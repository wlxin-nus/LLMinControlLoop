# -*- coding: utf-8 -*-
import json
import argparse
import re
from pathlib import Path
from typing import Dict, Any, List, Optional


def extract_section(text: str, title: str) -> Optional[str]:
    """
    ä»å¤šæ®µæ–‡æœ¬ä¸­æ ¹æ®æ ‡é¢˜æå–ç‰¹å®šéƒ¨åˆ†çš„å†…å®¹ã€‚
    ä¾‹å¦‚ï¼Œä» `... [TITLE]:\n content ...` ä¸­æå– `content`ã€‚

    Args:
        text: åŒ…å«å¤šä¸ªéƒ¨åˆ†çš„å®Œæ•´æ–‡æœ¬ã€‚
        title: è¦æå–çš„éƒ¨åˆ†çš„æ ‡é¢˜ (ä¾‹å¦‚, "CURRENT STATE")ã€‚

    Returns:
        æå–åˆ°çš„å†…å®¹å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› Noneã€‚
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ ‡é¢˜å’Œå…¶åçš„å†…å®¹
    # re.DOTALL ä½¿å¾— '.' å¯ä»¥åŒ¹é…åŒ…æ‹¬æ¢è¡Œç¬¦åœ¨å†…çš„ä»»æ„å­—ç¬¦
    pattern = re.compile(rf"\[{re.escape(title)}\]:\n(.*?)(?=\n\[[A-Z\s]+\]:|\Z)", re.DOTALL)
    match = pattern.search(text)
    if match:
        # .strip() ç”¨äºç§»é™¤å†…å®¹å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦
        return match.group(1).strip()
    return None


def format_llama_factory_entry(record: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°† memory_store ä¸­çš„å•æ¡è®°å½•è½¬æ¢ä¸º LLaMA-Factory çš„ Alpaca æ ¼å¼ã€‚

    Args:
        record:æ¥è‡ª memory_store['history'] çš„å•æ¡è®°å½•å­—å…¸ã€‚

    Returns:
        ä¸€ä¸ªç¬¦åˆ Alpaca æ ¼å¼çš„å­—å…¸ã€‚
    """
    # FIX: ç¡®ä¿å³ä½¿å€¼ä¸º null/Noneï¼Œä¹Ÿå°†å…¶è§†ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ TypeError
    llm_input_text = record.get("llm_input") or ""

    # 1. æå– [USER GOAL] å¹¶æ„å»º 'instruction'
    user_goal = extract_section(llm_input_text, "USER GOAL")
    instruction = (
        "You are a world-class building control AI expert. "
        "Your mission is to generate an optimal control action based on the provided context. "
        "Your primary objective is as follows: "
        f"{user_goal if user_goal else 'No specific goal provided.'}"
    )

    # 2. æå– [CURRENT STATE] å’Œ [RETRIEVED KNOWLEDGE] æ¥æ„å»º 'input'
    current_state = extract_section(llm_input_text, "CURRENT STATE")
    retrieved_knowledge = extract_section(llm_input_text, "RETRIEVED KNOWLEDGE")

    context_input = (
        f"**Current State Analysis:**\n{current_state if current_state else 'Not available.'}\n\n"
        f"**Relevant Knowledge:**\n{retrieved_knowledge if retrieved_knowledge else 'Not available.'}"
    )

    # 3. ç»„åˆ 'llm_thought' å’Œ 'action' æ¥æ„å»º 'output'
    # è¿™ç§XMLé£æ ¼çš„æ ¼å¼æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆè¡ŒåŠ¨ä¹‹é—´çš„ç»“æ„
    # FIX: ç¡®ä¿ thought å’Œ action ä¹Ÿæ˜¯ None-safe çš„
    thought = record.get("llm_thought") or ""
    action = record.get("action", {})
    # å°† action å­—å…¸è½¬æ¢ä¸ºç´§å‡‘çš„ JSON å­—ç¬¦ä¸²
    action_str = json.dumps(action, separators=(',', ':'))

    output = f"<think>{thought}</think>\n<action>{action_str}</action>"

    # 4. 'system' å­—æ®µç›´æ¥ä½¿ç”¨åŸå§‹çš„è¯¦ç»†æŒ‡ä»¤
    # FIX: ç¡®ä¿ system_prompt ä¹Ÿæ˜¯ None-safe çš„
    system_prompt = record.get("instruction") or ""

    # ç»„è£…æœ€ç»ˆçš„æ¡ç›®
    finetune_entry = {
        "instruction": instruction,
        "input": context_input,
        "output": output,
        "system": system_prompt,
        "history": []  # ç›®å‰æˆ‘ä»¬ä¸æ·»åŠ å¤šè½®å†å²ï¼Œä½†ä¿ç•™å­—æ®µä»¥ä¾¿æœªæ¥æ‰©å±•
    }

    return finetune_entry


def convert_memory_to_finetune_data(input_path: Path, output_path: Path):
    """
    è¯»å– memory_store JSON æ–‡ä»¶ï¼Œå°†å…¶è½¬æ¢ä¸º .jsonl æ ¼å¼ï¼Œå¹¶å†™å…¥è¾“å‡ºæ–‡ä»¶ã€‚
    """
    print(f"ğŸš€ Starting conversion from '{input_path}' to '{output_path}'...")

    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ Error: Input file not found at '{input_path}'")
        return
    except json.JSONDecodeError:
        print(f"âŒ Error: Could not decode JSON from '{input_path}'")
        return

    # memory_store çš„é¡¶å±‚é”®æ˜¯ run_idï¼Œæˆ‘ä»¬éœ€è¦è·å–å…¶å€¼
    if not isinstance(data, dict) or not data:
        print(f"âŒ Error: Expected a non-empty dictionary in '{input_path}'")
        return

    run_id = next(iter(data))
    history = data.get(run_id, {}).get("history", [])

    if not history:
        print("âš ï¸ Warning: No 'history' records found in the input file.")
        return

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)

    converted_count = 0
    with open(output_path, 'w', encoding='utf-8') as f:
        for record in history:
            # ç¡®ä¿è®°å½•æ˜¯æœ‰æ•ˆçš„å­—å…¸
            if not isinstance(record, dict):
                continue

            finetune_entry = format_llama_factory_entry(record)

            # å°†æ ¼å¼åŒ–åçš„å­—å…¸è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²å¹¶å†™å…¥æ–‡ä»¶ï¼Œæ¯æ¡è®°å½•å ä¸€è¡Œ
            f.write(json.dumps(finetune_entry, ensure_ascii=False) + '\n')
            converted_count += 1

    print(f"âœ… Conversion complete! Successfully converted {converted_count} records.")
    print(f"ğŸ“„ Your fine-tuning dataset is ready at: '{output_path}'")


if __name__ == "__main__":
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description="Convert building control simulation memory files to LLaMA-Factory ready fine-tuning datasets."
    )
    parser.add_argument(
        "--input_file",
        type=str,
        required=True,
        help="Path to the source memory_store JSON file."
    )
    parser.add_argument(
        "--output_file",
        type=str,
        required=True,
        help="Path to the destination .jsonl file for the fine-tuning data."
    )

    args = parser.parse_args()

    input_path = Path(args.input_file)
    output_path = Path(args.output_file)

    convert_memory_to_finetune_data(input_path, output_path)